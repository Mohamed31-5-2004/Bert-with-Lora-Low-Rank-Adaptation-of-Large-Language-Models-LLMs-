{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT with LoRA: Quora Paraphrase Detection\n",
    "\n",
    "This notebook demonstrates how to use BERT with LoRA for paraphrase detection using the Quora Question Pairs dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages (uncomment if running in Colab)\n",
    "!pip install torch transformers datasets peft"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess the Quora Question Pairs dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('quora')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(\n",
    "        example['questions']['text'][0],\n",
    "        example['questions']['text'][1],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "encoded = dataset.map(preprocess, batched=False)\n",
    "encoded = encoded.rename_column('is_duplicate', 'labels')\n",
    "encoded = encoded.remove_columns(['questions', 'id'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Setup: BERT + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_paraphrase',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_paraphrase',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded['train'],\n",
    "    eval_dataset=encoded['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def predict_paraphrase(q1, q2):\n",
    "    inputs = tokenizer(q1, q2, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    pred = torch.argmax(probs, dim=1).item()\n",
    "    return 'Paraphrase' if pred == 1 else 'Not paraphrase', probs.detach().numpy()\n",
    "\n",
    "# Example\n",
    "q1 = 'How can I be a good programmer?'\n",
    "q2 = 'What should I do to become a better programmer?'\n",
    "result, probabilities = predict_paraphrase(q1, q2)\n",
    "print(f'Result: {result}, Probabilities: {probabilities}')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}